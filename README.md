# ðŸš€ Jithendra Gunuru â€“ GCP Data Engineering Portfolio

**Data Engineer | Google Cloud Platform Specialist**

[LinkedIn](https://www.linkedin.com/in/jithendra-gunuru-gcp-data-engineer/) Email: [jithendra.gunuru2020@gmail.com](mailto\:jithendra.gunuru2020@gmail.com)

---

## ðŸŒŸ About This Portfolio

Welcome! This portfolio is a **real-world, enterprise-ready showcase** of my work as a Data Engineer specializing in **Google Cloud Platform**.\
Inside, you'll find fully-documented, production-grade examples of:

- **Batch Data Pipelines** (BigQuery, Dataproc, Airflow, PySpark)
- **Real-Time Streaming Pipelines** (Pub/Sub, Dataflow, Apache Beam)
- **End-to-End Cloud Data Architectures**
- **Infrastructure, Security, and Setup Guides**
- **Best Practices for Modern Data Engineering**

All projects are **modeled after real client use cases** from the healthcare, e-commerce, and analytics industries, demonstrating proficiency in both **ETL/ELT** and **streaming/real-time analytics**.

---

## ðŸ“‚ Portfolio Structure

```
/
â”œâ”€â”€ batch_pipelines/           # Batch ETL (PySpark, Dataproc, BigQuery, Airflow)
â”œâ”€â”€ streamline_pipeline/       # Real-time Streaming (Pub/Sub, Dataflow, BQ, GCS)
â”œâ”€â”€ data_engineer_setup/       # Workspace, VM, API, Infra & onboarding guides
â””â”€â”€ README.md                  # (This file)
```

---

## ðŸ† Project Highlights

### **Batch Pipelines (GCP/Enterprise ETL)**

- Automated extraction from MySQL â†’ GCS (CSV, Parquet) using PySpark and Dataproc
- Optimized BigQuery loads with partitioning, clustering, and cost-effective query design
- End-to-end orchestration with Apache Airflow (Cloud Composer)
- Realistic examples from e-commerce and healthcare batch analytics

### **Streamline (Streaming) Pipelines**

- Real-time ingestion from APIs to Pub/Sub
- Apache Beam (Dataflow) streaming:
  - Pub/Sub â†’ GCS (partitioned data lake)
  - Pub/Sub â†’ BigQuery (with audit columns, clustering, security)
- Producer and consumer scripts for true event-driven pipelines
- Partitioning, windowing, and cloud-native best practices

### **Data Engineer Workspace & Setup**

- API VM setup (Gunicorn/Flask) with GCP Secret Manager integration
- Secure firewall, IAM, and cloud onboarding scripts
- Terraform, Ansible, and Linux automation for cloud data engineering

---

## ðŸ’¼ Why This Portfolio Stands Out

- **Production-Ready:** Each component is built and documented as it would be in a real enterprise (auditable, scalable, secure)
- **Cloud-Native:** 100% GCP-centric with hands-on use of BigQuery, Pub/Sub, Dataflow, Composer, GCS, IAM, GSM, etc.
- **Breadth & Depth:** Covers both batch and streaming (end-to-end), with best practices and real error-handling/logging.
- **Collaboration-Ready:** All code is version-controlled (Git), readable, and comes with onboarding guides.
- **Direct Client Value:** Every example is based on what delivered actual business value for Fortune 500-like clients.

---

## ðŸ› ï¸ Skills & Technologies Demonstrated

- **Cloud:** Google Cloud Platform (BigQuery, Dataflow, GCS, Pub/Sub, Composer, Compute Engine), AWS, Azure
- **Programming:** Python, SQL, PySpark, Bash
- **Big Data:** Apache Spark, Apache Beam, Dataproc, Airflow (Composer)
- **DevOps:** Terraform, Ansible, Linux, systemd
- **Data Modeling:** Partitioning, clustering, audit columns, time travel, BQ views, and snapshots
- **Security:** IAM, GSM (Secret Manager), API Key management, audit logging
- **Collaboration:** Git, Agile, CI/CD
- **Visualization:** Looker Studio, Google Sheets

---

## ðŸš€ How to Use This Portfolio

Each folder contains:

- **README.md**: All steps and context to run or review the pipeline/component
- **Source code**: Production-ready scripts and configs
- **Troubleshooting & Q&A**: For interviews and live operations

**Want to see my approach in action?**

- Clone any folder, follow the readme, and youâ€™ll get a deployable data engineering artifact.

---

## ðŸ“ˆ What Youâ€™ll Actually See Here

- **GCP, For Real:** Every pipeline is hands-onâ€”designed, deployed, and managed on Google Cloud, not just code samples.
- **Serious Tools:** If youâ€™re a hiring manager, youâ€™ll spot BigQuery, Airflow, Apache Beam, PySpark, Pub/Sub, Dataflow, Linux automation, and modern DevOps woven throughout.
- **Cloud Engineering Mindset:** I care about structure, monitoring, auditability, and costâ€”not just shipping code. Youâ€™ll find IAM, Secret Manager, partitioned storage, and production-worthy patterns by default.
- **Built for Collaboration:** Every folder is readable, reproducible, and ready to be picked up by another engineer.
- **Always Growing:** Projects are updated with new tech and ideas. If you want something explained or see a gap, just askâ€”I treat this repo as my live workshop.

---

## ðŸ‘¨â€ðŸ’» About Me

- Hands-on builder passionate about cloud-native data engineering and analytics
- Experienced in delivering end-to-end GCP solutions for data-driven businesses
- Open to GCP Data Engineering collaborations and new opportunities

---

## ðŸ“« Contact

- **Email:** [jithendra.gunuru2020@gmail.com](mailto\:jithendra.gunuru2020@gmail.com)
- **LinkedIn:** [Jithendra Gunuru](https://www.linkedin.com/in/jithendra-gunuru-gcp-data-engineer/)

---

> **Thank you for visiting!**\
> Explore the projects, and reach out if you want to collaborate or discuss data engineering opportunities.

---

**Want to add project badges, project stats, or a one-line summary for each folder?**\
**Just askâ€”Bunny Bro will format it for you!**

